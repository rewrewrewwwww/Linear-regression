#### Mean Squared Error(MSE)
Средняя квадратичная ошибка 
$$ MSE = \frac{1}{n}\sum\limits_{i=1}^n(y_{i}-\hat{y}_{i})^{2} $$

#### Mean Absolute Error(MAE)
Средняя абсолютная ошибка 
$$ MAE = \frac{1}{n}\sum\limits_{i=1}^n|y_{i}-\hat{y}_{i}| $$
По сравнению с MSE, MAE хуже "борется" с выбросами

#### Root Mean Squared Error (RMSE)
Квадратный корень из среднеквадратичной ошибки
$$ RMSE = \sqrt{\frac{1}{n}\sum\limits_{i=1}^n(y_{i}-\hat{y}_{i})^{2}} $$
RMSE это по сути тот же MSE, только из него берут корень, чтобы размерность была схожа с исходными данными

#### Коэффициент $R^{2}$
Коэффициент детерминации
$$ R^{2} = 1 - \frac{\sum\limits_{i=1}^n(y_{i}-\hat{y}_{i})^{2}}{\sum\limits_{i=1}^n(y_{i}-\bar{y})^{2}}$$
- $\bar{y}$ - среднее значение целевой переменной
Это нормированная среднеквадратичная ошибка. Если она близка к единице, то модель хорошо объясняет данные, если же она близка к нулю, то прогнозы сопоставимы по качеству с константным предсказанием.

#### Mean Absolute Percentage Error (MAPE)
$$ MAPE = \frac{100}{n}\sum\limits_{i=1}^n|\frac{y_{i}-\hat{y}_{i}}{y_{i}}| $$
 Если получилось, например, что MAPE = 11.4%, то это говорит о том, что ошибка составила 11,4% от фактических значений. Основная проблема данной ошибки — нестабильность. Из-за деления на фактические значения эта метрика чувствительна к масштабу. Помимо этого, ошибка является несимметричной: одинаковые отклонения в плюс и в минус по-разному влияют на показатель ошибки.

 #### L1 регуляризация (или Lasso регрессия)
 $$ Loss = \frac{1}{n}\sum\limits_{i=1}^n(y_{i}-\hat{y}_{i})^{2} + \lambda_{1}\sum\limits_{j=1}^m|\omega_{j}|$$
 Здесь лямбда отвечает за то, насколько сильно регуляризация будет влиять на модель.

 $$ \nabla(LassoMSE) = \frac{2}{n}({\hat{Y}}-Y)X + \lambda_{1}sgn(W) $$

 #### L2 регуляризация (или Ridge регрессия)
 То же самое что и L1, только берется не модуль весов, а квадрат
$$ Loss = \frac{1}{n}\sum\limits_{i=1}^n(y_{i}-\hat{y}_{i})^{2} + \lambda_{2}\sum\limits_{j=1}^m\omega_{j}^{2}$$
$$ \nabla(RidgeMSE) = \frac{2}{n}({\hat{Y}}-Y)X + \lambda_{2}2W $$

#### ElasticNet
Это комбинация регуляризаций L1 и L2
$$ Loss = \frac{1}{n}\sum\limits_{i=1}^n(y_{i}-\hat{y}_{i})^{2} + \lambda_{1}\sum\limits_{j=1}^m+ \lambda_{2}\sum\limits_{j=1}^m\omega_{j}^{2}$$
$$ \nabla(ElasticNetMSE) = \frac{2}{n}({\hat{Y}}-Y)X + \lambda_{1}sgn(W) + \lambda_{2}2W $$

> Регуляризация обычно добавляется только к градиенту в процессе обновления весов модели, а не к значению ошибки. Это делается для того, чтобы учитывать штраф за большие значения весов в модели, но без изменения фактической ошибки, на которую мы настраиваемся. Таким образом, вместо добавления регуляризации к значению ошибки, мы обычно модифицируем градиент перед его применением к весам модели. Это позволяет управлять величиной штрафа, который мы налагаем на веса в зависимости от коэффициентов регуляризации.